import argparse, ast, json, logging, time
from pathlib import Path
from typing import List
import gc
import re
import pandas as pd
import torch
from tqdm import tqdm
from accelerate import Accelerator
from transformers import AutoProcessor
from transformers import Qwen2_5_VLForConditionalGeneration as ShotVLModel
from qwen_vl_utils import process_vision_info

HF_MODELS = {
    "ShotVL-3B": "Vchitect/ShotVL-3B",
    "ShotVL-7B": "Vchitect/ShotVL-7B",
}
DEFAULT_DATA_FILE = "evaluation/data/ShotBench/test.tsv"


def safe_load(text: str):
    text = text.strip()
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        return ast.literal_eval(text)

# Modified from https://github.com/open-compass/VLMEvalKit/blob/main/vlmeval/dataset/image_mcq.py
def build_prompt(row: pd.Series, root_dir: Path, default_fps: float):
    q = row["question"]
    options: dict[str, str] = safe_load(row["options"])
    opts_block = "Options:\n" + "\n".join(f"{k}. {v}" for k, v in options.items())
    prompt = (
        f"Question: {q}\n{opts_block}\n"
        "Please select the most likely answer from the options above."
    )

    prompt = (
        f"Question: {q}\n{opts_block}\n"
        "Please follow the steps below and select the most likely answer from the options above."
        "Step 1: Analyze the question.\n"
        "Step 2: Consider each option carefully.\n"
        "Step 3: Reason why each option is right or wrong.\n"
        "Step 4: Conclude with the correct answer."
    )

    

    # if row["category"] == "camera movement":
    #     prompt += (
    #         " The options are types of camera movements. "
    #         "Answer based on the CAMERA's motion (not object motion). "
    #         "Use the rule: background direction is opposite to camera movement."
    #     )

    # prompt = (
    #     f"Question: {q}\n{opts_block}\n"
    #     "Please answer in the following format, and output each section only ONCE:\n"
    #     "<think>\n"
    #     "Step 1: Provide a definition or explanation for each option above.\n"
    #     "Step 2: Summarize the key differences among the options and describe how to judge between them.\n"
    #     "Step 3: Analyze the given image based on these differences.\n"
    #     "Step 4: Select the most likely answer from the options, ensuring it is consistent with the reasoning process.\n"
    #     "At the end of <think>, clearly state: 'Therefore, the correct answer is X.'\n"
    #     "</think>\n"
    #     "<answer>\n"
    #     "X  # The answer here must exactly match the option stated in <think>.\n"
    #     "</answer>\n"
    #     "Do not output more than one <think> or <answer> block for this question. "
    #     "If the answers in <think> and <answer> do not match, your response will be considered incorrect."
    # )


    # prompt = (
    #     # ===== ç¤ºä¾‹ (demonstration) =====
    #     "Here is an example:\n\n"
    #     "Question: What's the composition of this shot?\n"
    #     "Options:\n"
    #     "A. Balanced, Right heavy\n"
    #     "B. Balanced, Center\n"
    #     "C. Center, Short side\n"
    #     "D. Center, Right heavy\n\n"
    #     "Step 1: Provide a definition or explanation for each option above.\n"
    #     "Step 2: Summarize the key differences among the options and describe how to judge between them.\n"
    #     "Step 3: Analyze the given image based on these differences.\n"
    #     "Step 4: Select the most likely answer from the options, ensuring it is consistent with the reasoning process.\n"
    #     "At the end of <think>, clearly state: 'Therefore, the correct answer is X.'\n"
    #     "Then in <answer>, repeat exactly that X.\n\n"
    #     "=== Demonstration Answer ===\n"
    #     "<think>The image shows a person standing in a room, facing slightly towards the right side of the frame. "
    #     "The composition centers around the individual, with no other significant objects or people visible. "
    #     "The background includes a plant and some furniture, but they do not detract from the main subject. "
    #     "Therefore, the composition is centered and slightly off-center to the right.\n\n"
    #     "Option A: Balanced, Right heavy - This option suggests a balanced composition where the right side is more prominent than the left. "
    #     "However, the image does not show any significant imbalance or emphasis on one side over the other.\n"
    #     "Option B: Balanced, Center - This option implies a balanced composition where both sides are equal in prominence. "
    #     "The image does not show any significant imbalance, so this option could be considered.\n"
    #     "Option C: Center, Short side - This option suggests a composition where the subject is centered and aligned with the short side of the frame. "
    #     "The image does not align the subject with the short side of the frame.\n"
    #     "Option D: Center, Right heavy - This option suggests a composition where the subject is centered and more prominently featured on the right side. "
    #     "The image does not show any significant emphasis on the right side over the center.\n\n"
    #     "Given the analysis, the most appropriate description for the composition is 'Center, Right heavy,' "
    #     "as the subject is centered but more prominently featured on the right side of the frame. "
    #     "Therefore, the correct answer is D.</think>\n"
    #     "<answer>D</answer>\n\n"
        
    #     # ===== ç›®æ ‡ä»»åŠ¡ (ç”¨æˆ·è¾“å…¥) =====
    #     "Now solve the following:\n\n"
    #     f"Question: {q}\n{opts_block}\n"
    #     "Step 1: Provide a definition or explanation for each option above.\n"
    #     "Step 2: Summarize the key differences among the options and describe how to judge between them.\n"
    #     "Step 3: Analyze the given image based on these differences.\n"
    #     "Step 4: Select the most likely answer from the options, ensuring it is consistent with the reasoning process.\n"
    #     "In <think>, explain your reasoning in detail. At the end of <think>, clearly state the conclusion once in the form: 'Therefore, the correct answer is X.'\n"
    #     "Do not repeat this conclusion multiple times.\n"
    #     "Close the reasoning with </think>.\n"
    #     "Then in <answer>, output only the letter X without any extra text, and close with </answer>."

    # )



    types = safe_load(row["type"])
    paths = safe_load(row["path"])
    msgs: List[dict] = []
    for t, rel_path in zip(types, paths):
        abs_path = (root_dir / rel_path).resolve()
        uri = f"file://{abs_path}"
        if t == "image":
            msgs.append({"type": "image", "image": uri})
        elif t == "video":
            msgs.append(
                {"type": "video", "video": uri, "max_pixels": 360 * 640, "fps": default_fps}
            )
        else:
            raise ValueError(f"Unsupported modality type: {t}")
    return msgs, prompt


def parse_args():
    p = argparse.ArgumentParser(description="Evaluate ShotVL on ShotBench (multi-GPU)")
    p.add_argument("--model", choices=HF_MODELS.keys(), default="ShotVL-3B")
    p.add_argument("--data-file", default=DEFAULT_DATA_FILE, help="TSV file path")
    p.add_argument("--root-dir", default=None)
    p.add_argument("--fps", type=float, default=12.0)
    p.add_argument("--reasoning", action="store_true")
    p.add_argument("--max-new-tokens", type=int, default=1024)
    p.add_argument("--output-dir", default="eval_results")
    p.add_argument("--category", default="composition")
    p.add_argument("--check_consistency", action="store_true", help="Whether to check and correct answer consistency between <think> and <answer>")
    return p.parse_args()

def resolve_inconsistency_with_model(answer, think_answer, answer_section, vision_msgs, row, processor, gen_model, args, accelerator):
    """
    å½“æ£€æµ‹åˆ°thinkå’Œanswerä¸ä¸€è‡´æ—¶ï¼Œè®©æ¨¡å‹åŸºäºå›¾åƒé‡æ–°åˆ¤æ–­å“ªä¸ªç­”æ¡ˆæ­£ç¡®
    """
    # æ„å»ºåˆ¤æ–­promptï¼Œè®©æ¨¡å‹åŸºäºå›¾åƒé€‰æ‹©æ­£ç¡®ç­”æ¡ˆå¹¶æä¾›ç†ç”±
    judge_prompt = f"""
Based on the image/video, I need you to determine which answer is correct.

The reasoning process concluded with: {think_answer}
But the final answer section shows: {answer_section}

Question: {row['question']}
Options: {safe_load(row['options'])}

Please analyze the image/video again and determine which answer ({think_answer} or {answer_section}) is correct based on the visual evidence.

Please provide your response in the following format:
<reasoning>
[Explain your analysis of the image/video and why you choose one answer over the other]
</reasoning>
<final_answer>
[Only the letter of the correct answer: A, B, C, D, etc.]
</final_answer>
"""
    
    # åˆ›å»ºåˆ¤æ–­çš„chat (åŒ…å«åŸå§‹å›¾åƒ/è§†é¢‘)
    judge_chat = [
        {"role": "system", "content": "You are a helpful assistant that determines the correct answer based on visual evidence. Always provide both reasoning and a final answer."},
        {"role": "user", "content": vision_msgs + [{"type": "text", "text": judge_prompt}]},
    ]

    print(f"=== Check Prompt ===")
    print("vision msg: ", vision_msgs)
    print("judge_prompt: ", judge_prompt)
    
    judge_text_in = processor.apply_chat_template(
        judge_chat, tokenize=False, add_generation_prompt=True
    )
    judge_img_in, judge_vid_in = process_vision_info(judge_chat)
    
    judge_inputs = processor(
        text=[judge_text_in],
        images=judge_img_in,
        videos=judge_vid_in,
        fps=args.fps,
        padding=True,
        return_tensors="pt",
    ).to(accelerator.device)
    
    # ç”Ÿæˆåˆ¤æ–­ç»“æœ
    judge_gen = gen_model.generate(
        **judge_inputs,
        max_new_tokens=200,  # å¢åŠ tokenæ•°é‡ä»¥å®¹çº³æ¨ç†è¿‡ç¨‹
        do_sample=False,
    )
    judge_trimmed = judge_gen[0][judge_inputs.input_ids.shape[-1]:]
    judge_result = processor.decode(
        judge_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    ).strip()
    
    print(f"=== Model Judgment (Full Response) ===")
    print(f"Model response: {judge_result}")
    
    # æå–æ¨ç†éƒ¨åˆ†å’Œæœ€ç»ˆç­”æ¡ˆ
    reasoning_match = re.search(r'<reasoning>(.*?)</reasoning>', judge_result, re.DOTALL | re.IGNORECASE)
    final_answer_match = re.search(r'<final_answer>\s*([A-G])\s*</final_answer>', judge_result, re.IGNORECASE)
    
    if reasoning_match:
        reasoning = reasoning_match.group(1).strip()
        print(f"=== Model Reasoning ===")
        print(f"Reasoning: {reasoning}")
    else:
        print("=== Could not extract reasoning from model response ===")
    
    # ä»æœ€ç»ˆç­”æ¡ˆä¸­æå–å­—æ¯
    if final_answer_match:
        final_answer = final_answer_match.group(1).upper()
        print(f"=== Model Final Decision ===")
        print(f"Final decision: {final_answer}")
        
        # ä½¿ç”¨æ¨¡å‹é€‰æ‹©çš„ç­”æ¡ˆ
        answer_start = answer.find("<answer>")
        answer_end = answer.find("</answer>") + 9
        
        if answer_start != -1 and answer_end != -1:
            new_answer_section = f"<answer>\n{final_answer}\n</answer>"
            updated_answer = answer[:answer_start] + new_answer_section + answer[answer_end:]
            print(f"=== Answer Corrected to: {final_answer} (based on model judgment) ===")
            
            # æ¸…ç†GPUå†…å­˜
            torch.cuda.empty_cache()
            return updated_answer
        else:
            print("Could not locate <answer> tags for correction")
    else:
        # å¦‚æœæ— æ³•ä»final_answeræ ‡ç­¾ä¸­æå–ï¼Œå°è¯•ä»æ•´ä¸ªå›å¤ä¸­æå–
        judge_match = re.search(r'([A-G])', judge_result.upper())
        
        if judge_match:
            final_answer = judge_match.group(1)
            print(f"=== Extracted answer from general response ===")
            print(f"Final decision: {final_answer}")
            
            # ä½¿ç”¨æ¨¡å‹é€‰æ‹©çš„ç­”æ¡ˆ
            answer_start = answer.find("<answer>")
            answer_end = answer.find("</answer>") + 9
            
            if answer_start != -1 and answer_end != -1:
                new_answer_section = f"<answer>\n{final_answer}\n</answer>"
                updated_answer = answer[:answer_start] + new_answer_section + answer[answer_end:]
                print(f"=== Answer Corrected to: {final_answer} (based on model judgment) ===")
                
                # æ¸…ç†GPUå†…å­˜
                torch.cuda.empty_cache()
                return updated_answer
            else:
                print("Could not locate <answer> tags for correction")
        else:
            # å¦‚æœæ— æ³•ä»æ¨¡å‹åˆ¤æ–­ä¸­æå–ç­”æ¡ˆï¼Œé»˜è®¤ä½¿ç”¨thinkä¸­çš„ç­”æ¡ˆ
            print(f"=== Could not parse model judgment, using think answer: {think_answer} ===")
    
    # å›é€€åˆ°ä½¿ç”¨thinkä¸­çš„ç­”æ¡ˆ
    answer_start = answer.find("<answer>")
    answer_end = answer.find("</answer>") + 9
    
    if answer_start != -1 and answer_end != -1:
        new_answer_section = f"<answer>\n{think_answer}\n</answer>"
        updated_answer = answer[:answer_start] + new_answer_section + answer[answer_end:]
        print(f"=== Answer Corrected to: {think_answer} ===")
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
        return updated_answer
    else:
        print("Could not locate <answer> tags for correction")
        return answer

def main():
    args = parse_args()
    accelerator = Accelerator()
    repo_id = HF_MODELS[args.model]
    root_dir = Path(args.root_dir) if args.root_dir else Path(args.data_file).parent

    accelerator.print(f"ğŸ”„ Loading {repo_id} ...")
    model = ShotVLModel.from_pretrained(
        repo_id,
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )
    model.eval()
    model.generation_config.temperature = None
    model = accelerator.prepare(model)

    processor = AutoProcessor.from_pretrained(repo_id)

    full_df = pd.read_csv(args.data_file, sep="\t")
    world_size, rank = accelerator.num_processes, accelerator.process_index
    local_df = full_df.iloc[rank::world_size].reset_index(drop=True)
    local_df["prediction"] = ""

    out_dir = Path(args.output_dir) / args.model
    out_dir.mkdir(parents=True, exist_ok=True)
    ts = int(time.time())
    logging.basicConfig(
        filename=out_dir / f"run_{ts}_rank{rank}.log",
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    system_prompt = (
        "A conversation between User and Assistant. The user asks a question, "
        "and the Assistant solves it. The assistant first thinks about the "
        "reasoning process in the mind and then provides the user with the "
        "answer. The reasoning process and answer are enclosed within <think> "
        "</think> and <answer> </answer> tags."
        if args.reasoning
        else "You are a helpful assistant."
    )

    iterable = tqdm(local_df.iterrows(), total=len(local_df), disable=not accelerator.is_main_process)

    with torch.inference_mode():
        gen_model = accelerator.unwrap_model(model)
        for idx, row in iterable:
            if row["category"] != args.category and args.category != "all": 
                continue
            print("row: ", row)
            # quit()
            vision_msgs, prompt = build_prompt(row, root_dir, args.fps)
            print("vision_msgs: ", vision_msgs)
            print("prompt: ", prompt)

            chat = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": vision_msgs + [{"type": "text", "text": prompt}]},
            ]
            text_in = processor.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
            img_in, vid_in = process_vision_info(chat)


            # inputs = processor(
            #     text=[text_in],
            #     images=img_in,
            #     videos=vid_in,
            #     fps=args.fps,
            #     padding=True,
            #     return_tensors="pt",
            # ).to(accelerator.device)

            # gen = gen_model.generate(
            #     **inputs,
            #     max_new_tokens=args.max_new_tokens,
            #     do_sample=False,
            # )
            # trimmed = gen[0][inputs.input_ids.shape[-1]:]
            # answer = processor.decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)

            # print("\n\n=== Predicted Answer ===")
            # print(answer)
            # print("=== End ===\n\n")


            # local_df.at[idx, "prediction"] = answer
            # logging.info(answer)
            # torch.cuda.empty_cache()
            # gc.collect()
            # ...existing code...

            inputs = processor(
                text=[text_in],
                images=img_in,
                videos=vid_in,
                fps=args.fps,
                padding=True,
                return_tensors="pt",
            ).to(accelerator.device)

            gen = gen_model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                do_sample=False,
            )
            trimmed = gen[0][inputs.input_ids.shape[-1]:]
            answer = processor.decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)

            print("\n\n=== Initial Predicted Answer ===")
            print(answer)
            print("=== End Initial Answer ===\n\n")
            # ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®æ­£
            if args.check_consistency:
                
                def extract_think_answer(text, processor, gen_model, accelerator):
                    """ä½¿ç”¨æ¨¡å‹ä»æ–‡æœ¬ä¸­æå–<think>éƒ¨åˆ†çš„ç­”æ¡ˆ"""
                    # æå–<think>éƒ¨åˆ†
                    think_match = re.search(r'<think>(.*?)</think>', text, re.DOTALL | re.IGNORECASE)
                    if not think_match:
                        return None
                    
                    think_content = think_match.group(1)
                    
                    # æ„å»ºæå–ç­”æ¡ˆçš„prompt
                    extract_prompt = f"""
                Please extract the final answer from the following reasoning text. Look for statements that conclude with a specific answer choice (like A, B, C, D, etc.).

                Reasoning text:
                {think_content}

                Please respond with only the letter of the answer (A, B, C, D, etc.) that this reasoning concludes with. If no clear answer is stated, respond with "NONE".
                """
                    
                    # åˆ›å»ºæå–ç­”æ¡ˆçš„chat
                    extract_chat = [
                        {"role": "system", "content": "You are a helpful assistant that extracts answers from reasoning text."},
                        {"role": "user", "content": [{"type": "text", "text": extract_prompt}]},
                    ]
                    
                    extract_text_in = processor.apply_chat_template(
                        extract_chat, tokenize=False, add_generation_prompt=True
                    )
                    
                    extract_inputs = processor(
                        text=[extract_text_in],
                        images=None,
                        videos=None,
                        padding=True,
                        return_tensors="pt",
                    ).to(accelerator.device)
                    
                    # ç”Ÿæˆæå–ç»“æœ
                    extract_gen = gen_model.generate(
                        **extract_inputs,
                        max_new_tokens=50,
                        do_sample=False,
                    )
                    extract_trimmed = extract_gen[0][extract_inputs.input_ids.shape[-1]:]
                    extract_result = processor.decode(
                        extract_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
                    ).strip()
                    
                    print(f"Model extracted from think: {extract_result}")
                    
                    # ä»æ¨¡å‹å›å¤ä¸­æå–ç­”æ¡ˆ
                    # import re
                    answer_match = re.search(r'([A-G])', extract_result.upper())
                    
                    if answer_match and "NONE" not in extract_result.upper():
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        return answer_match.group(1).upper()
                    else:
                        # æ¸…ç†GPUå†…å­˜
                        torch.cuda.empty_cache()
                        return None
                                
                def extract_answer_section(text):
                    """ä»æ–‡æœ¬ä¸­æå–<answer>éƒ¨åˆ†çš„ç­”æ¡ˆ"""
                    # æå–<answer>éƒ¨åˆ†
                    answer_match = re.search(r'<answer>\s*([A-G])\s*</answer>', text, re.IGNORECASE)
                    if answer_match:
                        return answer_match.group(1).upper()
                    
                    return None
                
                # æå–thinkå’Œansweréƒ¨åˆ†çš„ç­”æ¡ˆ
                # think_answer = extract_think_answer(answer)
                think_answer = extract_think_answer(answer, processor, gen_model, accelerator)
                answer_section = extract_answer_section(answer)
                
                print(f"\n=== Consistency Check ===")
                print(f"Thinkç­”æ¡ˆ: {think_answer}")
                print(f"Answeréƒ¨åˆ†ç­”æ¡ˆ: {answer_section}")
                
                # æ£€æŸ¥ä¸€è‡´æ€§
                if think_answer and answer_section:
                    if think_answer == answer_section:
                        print("=== Answer is consistent ===")
                    else:
                        print(f"=== Inconsistency Detected ===")
                        print(f"Thinkéƒ¨åˆ†ç»“è®º: {think_answer}")
                        print(f"Answeréƒ¨åˆ†: {answer_section}")
                        print(f"Using answer from <think> section: {think_answer}")
                        
                        # æ›¿æ¢answeréƒ¨åˆ†ä¸ºthinkä¸­çš„ç­”æ¡ˆ
                        answer_start = answer.find("<answer>")
                        answer_end = answer.find("</answer>") + 9

                        if answer_start != -1 and answer_end != -1:
                            # è°ƒç”¨å‡½æ•°æ¥è§£å†³ä¸ä¸€è‡´é—®é¢˜
                            answer = resolve_inconsistency_with_model(
                                answer, think_answer, answer_section, vision_msgs, row,
                                processor, gen_model, args, accelerator
                            )
                        else:
                            print("Could not locate <answer> tags for correction")
                else:
                    if not think_answer:
                        print("è­¦å‘Šï¼šæ— æ³•ä»<think>éƒ¨åˆ†æå–ç­”æ¡ˆ")
                    if not answer_section:
                        print("è­¦å‘Šï¼šæ— æ³•ä»<answer>éƒ¨åˆ†æå–ç­”æ¡ˆ")
                        
                        # å¦‚æœæ— æ³•æå–answeréƒ¨åˆ†ï¼Œä»thinkä¸­æå–ç­”æ¡ˆå¹¶é‡æ–°æ„å»ºresponse
                        if think_answer:
                            print(f"ä»<think>éƒ¨åˆ†æå–åˆ°ç­”æ¡ˆ: {think_answer}ï¼Œé‡æ–°æ„å»ºresponse")
                            
                            # æå–åŸå§‹çš„thinkéƒ¨åˆ†
                            think_match = re.search(r'<think>(.*?)</think>', answer, re.DOTALL | re.IGNORECASE)
                            if think_match:
                                think_content = think_match.group(1)
                                
                                # é‡æ–°æ„å»ºå®Œæ•´çš„response
                                new_response = f"<think>{think_content}</think>\n<answer>\n{think_answer}\n</answer>"
                                answer = new_response
                                print(f"=== é‡æ–°æ„å»ºçš„response ===")
                                print(answer)
                                print("=== é‡æ–°æ„å»ºå®Œæˆ ===")
                                
                                # é‡æ–°æå–answer_sectionä»¥ä¾¿åç»­å¤„ç†
                                answer_section = think_answer
                                    
                print("=== End Consistency Check ===\n")


            # # ä¸€è‡´æ€§æ£€æŸ¥å’Œä¿®æ­£
            # if "<think>" in answer and "<answer>" in answer and args.check_consistency:
            #     # æ„å»ºä¸€è‡´æ€§æ£€æŸ¥çš„prompt
            #     consistency_prompt = f"""
            #         Please carefully review the following response and check if the reasoning in <think> and the final answer in <answer> are consistent.

            #         {answer}

            #         Your task:
            #         1. Extract the conclusion from the <think> section (look for statements like "Therefore, the correct answer is X")
            #         2. Extract the answer from the <answer> section
            #         3. Check if they match exactly
            #         4. If they are consistent, respond with: CONSISTENT
            #         5. If they are inconsistent, respond with: INCONSISTENT - The answer should be [X] based on the reasoning in <think>

            #         Please respond with only one of these formats:
            #         - CONSISTENT
            #         - INCONSISTENT - The answer should be [X] based on the reasoning in <think>
            #         """

            #     # åˆ›å»ºä¸€è‡´æ€§æ£€æŸ¥çš„chat
            #     consistency_chat = [
            #         {"role": "system", "content": "You are a helpful assistant that checks reasoning consistency."},
            #         {"role": "user", "content": [{"type": "text", "text": consistency_prompt}]},
            #     ]
                
            #     consistency_text_in = processor.apply_chat_template(
            #         consistency_chat, tokenize=False, add_generation_prompt=True
            #     )
                
            #     consistency_inputs = processor(
            #         text=[consistency_text_in],
            #         images=None,
            #         videos=None,
            #         padding=True,
            #         return_tensors="pt",
            #     ).to(accelerator.device)
                
            #     # ç”Ÿæˆä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
            #     consistency_gen = gen_model.generate(
            #         **consistency_inputs,
            #         max_new_tokens=100,
            #         do_sample=False,
            #     )
            #     consistency_trimmed = consistency_gen[0][consistency_inputs.input_ids.shape[-1]:]
            #     consistency_result = processor.decode(
            #         consistency_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            #     )
                
            #     print(f"\n=== Consistency Check Result ===")
            #     print(f"Consistency Result: {consistency_result}")
                
            #     # å¦‚æœä¸ä¸€è‡´ï¼Œä¿®æ­£ç­”æ¡ˆ
            #     if "INCONSISTENT" in consistency_result.upper():
            #         import re
                    
            #         # ä»ä¸€è‡´æ€§æ£€æŸ¥ç»“æœä¸­æå–æ­£ç¡®ç­”æ¡ˆ
            #         corrected_match = re.search(r'The answer should be \[?([A-G])\]?', consistency_result, re.IGNORECASE)
                    
            #         if corrected_match:
            #             corrected_answer = corrected_match.group(1)
            #             print(f"Detected inconsistency. Correcting answer to: {corrected_answer}")
                        
            #             # æ›¿æ¢answeréƒ¨åˆ†
            #             answer_start = answer.find("<answer>")
            #             answer_end = answer.find("</answer>") + 9
                        
            #             if answer_start != -1 and answer_end != -1:
            #                 new_answer_section = f"<answer>\n{corrected_answer}\n</answer>"
            #                 answer = answer[:answer_start] + new_answer_section + answer[answer_end:]
            #                 print(f"=== Corrected Answer ===")
            #             else:
            #                 print("Could not locate <answer> tags for correction")
            #         else:
            #             print("Could not extract corrected answer from consistency check")
            #     else:
            #         print("=== Answer is consistent ===")
                
            #     # print("=========================")
                
            #     # æ¸…ç†GPUå†…å­˜
            #     torch.cuda.empty_cache()

            print("\n\n=== Final Predicted Answer ===")
            print(answer)
            print("=== End ===\n\n")

            local_df.at[idx, "prediction"] = answer
            print("answer add to excel")
            logging.info(answer)
            torch.cuda.empty_cache()
            gc.collect()

# ...existing code...

    json_str = local_df.to_json(orient="split", index=False)
    all_json = accelerator.gather_for_metrics([json_str], use_gather_object=True)
    
    if accelerator.is_main_process:
        merged = pd.concat(
            (pd.read_json(s, orient="split") for s in all_json),
            ignore_index=True
        ).sort_index()
        out_path = out_dir / f"predictions_{ts}.xlsx"
        merged.to_excel(out_path, index=False)
        accelerator.print(f"âœ… Done! Results saved to {out_path}")

if __name__ == "__main__":
    main()
